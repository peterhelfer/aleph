Project Aleph

Aim: Develop an intelligent system architecture

     Influences:
     - Lindsay & Norman, Review of Human information processing
     - Baar/Dehaene's Global workspace theory
     - Barsalou's Perceptual symbol systems
     - Dennett's fame-in-the-brain/cerebral-celebrity

     Features:
     - Layers of simulators
     - bottom-up perception/top-down "attenuation"
     - bottom-up/top-down loops can sustain simulation without input
     - Lateral inhibition implements competition for consciousness
     - Attention...?
     - Resistance to catastrophic interference
     - time-series ability

     Technology:

     - LSM-style spiking, recurrent modules (experts)
     - Three-factor (neo-hebbian) learning (3FL)
     - Reinforcement learning?
     - Pseudo-rehearsal

     - word (or whatever) embedding


     Plan:

     - Implement a small recurrent spiking architecture with neo-hebbian
       learning. Overall network activity signifies recognition. Train on
       simple time-series task (FSDD?). 

     - Then: extend to layers of recognisers/simulators

     Study:

     - LSMs that reproduce/complete time-series samples
       ==> There are ones that are trained on a time series produced by a
           particular generator (e.g. Mackey-Glass), and are then able to
           complete other samples generated by *the same* generator.
       
     - Has 3FL been used with LSMs?
     - How are hopfield nets trained?
       ==> To train each pattern, set the activations to zero/one and apply
           hebbian, i.e. increase weights between active units. This tends
           to create attractors at the "trained" patterns.

======= 2023-12-15 =======

I'm thinking of little RNNs that learn to "resonate" with particular input
sequences. How do they learn? They start out random, self-reinforce and
laterally-inhibit to diversify. And strengthen input connections that get them
going.

What is meant by "resonate" and "self-reinforce"?

Resonate: I'm thinking overall spiking activity level. And the RNN could
have an "output neuron" that receives input from all the RNN neurons, so that
its activity level is a measure of the RNN's activity level. Other RNNs
receive input from such "output neurons".

Self-reinforce: I want to think hebbian or neo-hebbian here.

Lateral inhibition: Each RNN could have a neuromodulator neuron that could
be excited or inhibited by other RNNs' output neurons.
--------------------
Let's cook up some random spiking RNNs with activity indicators, feed them some time
series and see if they react differently.

What kind of technology shall we use? Let's start with Numpy...

======= 2024-01-22 =======
Somewhere along the line I ran into predictive coding (or processing) as
described in
- Andy Clark's book Surfing Uncertainty: Prediction, Action, and the
  Embodied Mind
Reviewed by
* Scott Alexander: https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/
Recent paper:
- Rao et al 2024: Active Predictive Coding
which builds on earlier works:
- Jiang & Rao 2022, Predictive Coding Theories of Cortical Function
- Rao & Ballard 1999 Predictive coding
- Rao 1999 An optimal estimation approach to visual perception and learning
    ==> This is by far the best explanation. Also, it really helped to learn
        about Kalman filters first. Babb 2015 is the best explanation for that.


This is all very cool stuff: higher layers generate predictions, send them
to lower layers; lower layers compute difference between these predictions
and actual input and send error signals back up; up and down it goes, and
confidence levels are calculated and transmitted together in both
directions.

But I found that I needed to brush up on fundamentals:
* Hopfield 1982
* Hinton & Sejnowski 1983, Optimal (Boltzmann machine)
- Hinton 2002 Training Products of Experts by Minimizing Contrastive
  Divergence (RBMs)
- Hinton & Salachutdinov 2006, Reducing dimensionality (RBM-based auto-encoder)
- Hinton 2006 Wake-Sleep
- Freund & Heussler 1992 Unsupervided Learning... (investgations of RBMs)
- Hinton 2007 Learining Multiple Layers

Also:
- Smolensky 1986, Harmony Theory (in PDP vol 1, p 194)
- Grush 2004, Emulation Theory
- Friston: multiple works cited by And Clark
- Ransom & Fazelpour 2015 Three Problems (also cited by Andy Clark)
- Babb 2015 How a Kalman filter works, in pictures

======= 2024-01-24 =======

So I now undestand how the Hopfield net (HN)works (how the Hebbian learning
rule produces energy minima and how the update rule finds local minima. The
update rule sets units to 0 or 1 according to if net input is negative or not.

Also, I understand how the Boltzmann machine (BM) works: it is like a HN but
with stochastic units that go 0/1 with probability according to a logistic
function applied to their net input x: p = 1 / (1 + exp(-x/T)). When T
approaches 0, the logistic curve becomes the deterministic step function of
a HN.

Next: The restricted BM, RBM. Here the units are divided into input (V) and
hidden (H) layers, and there are no intra-layer connections. The same weight
matrix W is used for both directions, encoding (extraction) and decoding
(reconstruction:

      extraction:     h = σ(V^T * W + β)        Equation 1
      reconstruction: v = σ(W * H + α)          Equation 2

where V^T is the transpose of V (i.e. a row vector) and α and β are bias vector.

Note that the activation function σ is stochastic: Eq 1 calculates the
probabilities that the elements of h be assigned the value 1 rather than 0.

Training: The RBM is trained by contrastive divergence. The goal is to, for
each input pattern V0, minimize the expected error in the reconstructed
pattern V1, as determined by applying Eqs. 1 and 2 above, for the current W
(which is randomly initialized).

To find the expected error in V1, we need its probability distribution,
which can be calculated as the sum of the energies of all states (V1 + H1)
where V0 = V1, devided by the sum of the energies of all states.

This will be a function of W. So then we can calculate the gradient of the
error wrt W and reduce it by gradient descent.

Fortunately, it turns out that this gradient can be simply calculated as the
difference between the expected value of <v_i h_j> with clamped input
("data") and when free-running ("model").

These expected values are (apparently) hard to calculate, but can be
obtained by sampling the state of the network, using Gibbs sampling and what
not. I need to read a bit more to understand all this.

======= 2024-01-28 =======

Having trouble getting a good understang of RBMs. I tried reading Hinton &
Salachutdinov 2006 and Freund 7 heussler 1992, but they are hard to digest.

Shahar Siegman's blog post on RBMs (http://tinyurl.com/39r28yft) was good,
but it doesn't explain the training alrorithm. I'll follow some of his
references:

* tanhyp = sigmoid2 * logistic - 1,  http://tinyurl.com/4eubfca2
* Nagesh Sing Intro to RBMs http://tinyurl.com/mr2rrxwk
    -- this was helpful, though not very well written and with inaccuracies
* Frank Noe's RBM video http://bit.ly/3HDbCUT
    -- better, still the crucial step from energy formula contrastive
       divergence remains obscure.

* Hinton's lecture series "Neural Nets for Machine Learning" is really good.

======= 2024-02-06 =======

Metropolis sampling: If we have a probability distribution given by
   p(x) = k * f(x), where f is known but k is not, then we can't directly
   sample from the target distribution p. Well, you say, why don't we just
   integrate f over its domain, then k must be the inverse of this result
   since the integral of p must be 1. Sure, but the integral may be
   intractable. Metroplolis sampling to the rescue: we define an MCMC
   (Monte Carlo Markov chain) whose stationary distribution is p, and use
   this chain to generate samples! How the heck do we do this? Like this:

   1. Select an arbitrary starting point x_0

   2. Select a "proposal distribution" q(x* | x_n) from which we sample a
      candidate x* given x_n. For Metropolis sampling, the proposal
      distribution must be symmetrical around x_n-1, e.g. a Gaussian;
      Metropolis-Hastings sampling is a generalization that permits
      asymmetrical distributions. 

   3. Generate consecutive samples by first sampling from the proposal
      distribution to get a candidate x*, then accept or reject x* as the
      next sample with an acceptance probability
      A(x* | x_n) = min(1, p(x*)/p(x_n))

      So we always accept steps that take us towards higher probability
      densities in p and the probability of stepping towards lower densities
      diminishes with the size of the density drop. As a result, the MCMC
      spens more time in denser regions of p, and it can be shown
      (apparently) that its stationary distribution equals p.

      Very nice!

      Metropolis-Hastings sampling only differs in that the acceptance
      function is min(1, p(x*)/p(x_n) * q(x_n|x*) / q(x*/x_n)).

Gibbs sampling: Here we have multiple random variables x_1, x_2, x_3 and we
      generate successive values by repeatedly drawing from the conditional
      distributions of each variable given all the others:

      x_1(t) ~ P(x_1 | x_2(t-1), x_3(t-1))
      x_2(t) ~ P(x_2 | x_1(t),   x_3(t-1))
      x_3(t) ~ P(x_3 | x_1(t),   x_2(t))

      So there is no accept/reject, but all those conditional distributions
      must be known.

======= 2024-02-08 =======

Maximum Likelihood Estimate (MLE): Given some observed data and a parameterized
    model assumed to have generated the data, find a distribution and a set
    of parameters that maximize the likelihood of the data. In machine
    learning: find the model parameter values that maximize the likelihood
    of the data, i.e. find theta that maximizes P(X|theta)

Maximum A Posteriori Probability (MAP) estimation: Unlike MLE, explicitly
    takes priors into account: Maximize P(theta|X) where
    P(theta|X) = P(X|theta) * P(theta) / P(X)


Deep Belief Networks



Predictive coding approximates backprop along arbitrary computation graphs

======= 2024-04-05 =======
Reading up existing implementation of predicting coding models, as found at
paperswithcode.com:

- Lotter et al. 2017: prednet
    - forward (error signaling) and backward (prediction) layers implemented
      by convolutional networks.
    - applied to video frame prediction (moving cars and tumbling heads)
    ==> Replacing the lightweight layers in Rao's architecture with heavy
        CNNs may improve performance, but makes this architecture unsuitable
        for neuromorphic impelmentation.
    
- van den Oord et al. 2019: Contrastive Predictive Coding
    ==> While this work does cite Rao & Ballard and Friston, their algorithm
        does not seem to have anything to do with it, except the name.

======= 2024-06-03 =======

While reading the excellent Rao 1999, I found I had to make a detour to
understand Kalman filters. I found an excellent exposition in Babb 2015.
I'll try to implement according to that model.


